# -*- coding: utf-8 -*-
"""Stock market prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wH1mWHLFvYDTOviJIEILtfzz5NPF8Td1
"""

!pip install pandas-datareader

import pandas_datareader as pdr
import pandas as pd
from datetime import datetime

import numpy as np

df= pd.read_csv('Stock.csv')

df.head()

"""Transforming to lowercase for convenince"""

df.tail()

df.columns = df.columns.str.strip().str.lower()
df.columns = df.columns.str.strip().str.replace(' ', '_').str.lower()

print(df.columns)

print(df.index)

"""Making 'date' has a colum"""

df.reset_index(inplace=True)
print(df.head())

"""CHECKING IF NULL VALUE PRESENCE

"""

print(df.isnull().sum())

df['date'] = pd.to_datetime(df['date'])

"""Removing M or B alphabates from voulume colum has machine doesn't understand aphbhates

"""

def convert_trading_volume(volume):
    if 'M' in volume:
        return float(volume.replace('M', '')) * 1e6
    elif 'B' in volume:
        return float(volume.replace('B', '')) * 1e9
    else:
        return float(volume)

df['trading_volume'] = df['trading_volume'].apply(convert_trading_volume)

df['percentage_change_rate'] = df['percentage_change_rate'].str.replace('%', '').astype(float)

df.drop_duplicates(inplace=True)

from scipy import stats
import numpy as np

df = df[(np.abs(stats.zscore(df[['closing_price', 'opening_price', 'high_price', 'low_price']])) < 3).all(axis=1)]



df = df.sort_values(by='date')

df = df[(df['closing_price'] >= 0) & (df['opening_price'] >= 0)]

df['daily_return'] = df['closing_price'].pct_change()

df.head()

# Remove or fill NaN values
df.dropna(inplace=True)

df['trading_volume'] = pd.to_numeric(df['trading_volume'], errors='coerce')

df['daily_return'] = df['closing_price'].pct_change()

print(df.info())

df.head()

summary_stats = df[['closing_price', 'opening_price', 'high_price', 'low_price', 'trading_volume', 'percentage_change_rate']].describe()
print(summary_stats)



skewness = df[['closing_price', 'opening_price', 'high_price', 'low_price', 'trading_volume']].skew()
kurtosis = df[['closing_price', 'opening_price', 'high_price', 'low_price', 'trading_volume']].kurtosis()
print('Skewness:\n', skewness)
print('Kurtosis:\n', kurtosis)

"""**Closing** **price**, **Opening** **price** , **high** **price** and **low** **price** - having positive skewness, it suggests that there may be som days of when prices spiked higher than average, which is common in stock data.

**Trading** **Volume**-
Positive skewness in trading volume might suggest that there are certain days with exceptionally high trading activity, possibly due to market events or news.
"""

df['high_price'].plot(figsize=(12,4))

import matplotlib.pyplot as plt
plt.figure(figsize=(12, 4))
plt.plot(df['date'], df['closing_price'], label='Closing Price', color='blue')
plt.xlabel('Date')
plt.ylabel('Closing Price')
plt.title('Closing Price Over Time')
plt.legend()
plt.show()

df['moving_avg_30'] = df['closing_price'].rolling(window=30).mean()  # 30-day moving average
plt.figure(figsize=(12, 4))
plt.plot(df['date'], df['closing_price'], label='Closing Price', color='blue')
plt.plot(df['date'], df['moving_avg_30'], label='30-Day Moving Average', color='orange')
plt.xlabel('Date')
plt.ylabel('Price')
plt.title('Closing Price and 30-Day Moving Average')
plt.legend()
plt.show()

plt.figure(figsize=(10, 6))
plt.bar(df['date'], df['trading_volume'], color='green')
plt.xlabel('Date')
plt.ylabel('Trading Volume')
plt.title('Trading Volume Over Time')
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(df['date'], df['daily_return'], label='Daily Return', color='red')
plt.xlabel('Date')
plt.ylabel('Daily Return')
plt.title('Daily Return Over Time')
plt.legend()
plt.show()

plt.figure(figsize=(8, 6))
plt.hist(df['daily_return'].dropna(), bins=50, color='purple')
plt.xlabel('Daily Return')
plt.ylabel('Frequency')
plt.title('Distribution of Daily Returns')
plt.show()

import seaborn as sns
plt.figure(figsize=(8, 6))
sns.heatmap(df[['closing_price', 'opening_price', 'high_price', 'low_price', 'trading_volume']].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

plt.figure(figsize=(8, 6))
sns.boxplot(data=df[['closing_price', 'opening_price', 'high_price', 'low_price']])
plt.title('Price Range Box Plot')
plt.show()

import pandas as pd

# Assuming df is your DataFrame
# Define a function to identify outliers
def identify_outliers(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Identify outliers
    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]

    return outliers

# Identify outliers for key variables
outlier_columns = ['closing_price', 'opening_price', 'high_price', 'low_price']
outlier_results = {col: identify_outliers(df, col) for col in outlier_columns}

# Print the number of outliers and their values
for col, outliers in outlier_results.items():
    print(f'Outliers in {col}:')
    print(outliers)
    print(f'Number of outliers: {len(outliers)}\n')

"""REMOVING OUTLIERS"""

def is_outlier(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR


    return (data[column] < lower_bound) | (data[column] > upper_bound)

outlier_mask = pd.Series(False, index=df.index)

for col in outlier_columns:
    outlier_mask |= is_outlier(df, col)


filtered_df = df[~outlier_mask]

print(f'Original number of rows: {len(df)}')
print(f'Number of rows after removing outliers: {len(filtered_df)}')

print("Summary Statistics After Removing Outliers:\n", filtered_df[['closing_price', 'opening_price', 'high_price', 'low_price']].describe())

# Boxplot after removing outliers
plt.figure(figsize=(12, 6))
sns.boxplot(data=filtered_df[['closing_price', 'opening_price', 'high_price', 'low_price']])
plt.title('Boxplot After Removing Outliers')
plt.show()

"""We succesfully filtered out **229** outlier from the **data**"""

import scipy.stats as stats

# Q-Q plot for closing price
plt.figure(figsize=(8, 6))
stats.probplot(df['closing_price'], dist="norm", plot=plt)
plt.title('Q-Q Plot for Closing Price')
plt.show()

df = filtered_df

df['rolling_mean'] = df['closing_price'].rolling(window=30).mean()
df['rolling_std'] = df['closing_price'].rolling(window=30).std()
plt.plot(df['date'], df['rolling_mean'], label='30-Day Rolling Mean')
plt.plot(df['date'], df['rolling_std'], label='30-Day Rolling Std Dev')
plt.legend()
plt.show()

"""30-day rolling mean and standard deviation of closing price shows significant fluctuation, it indicates that the stock price has experienced considerable volatility over time"""

pd.to_datetime(df['date'])

df['volatility_30'] = df['daily_return'].rolling(window=30).std()  # 30-day rolling volatility
plt.figure(figsize=(10, 6))
plt.plot(df['date'], df['volatility_30'], label='30-Day Volatility', color='orange')
plt.xlabel('Date')
plt.ylabel('Volatility')
plt.title('30-Day Rolling Volatility')
plt.legend()
plt.show()



"""Data have lot of fluctation and it is highly volatail, this will affect the model prediction leading to unaccurate result


"""

import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

plt.figure(figsize=(12, 6))
plt.subplot(211)
plot_acf(df['daily_return'].dropna(), lags=40, ax=plt.gca())
plt.subplot(212)
plot_pacf(df['daily_return'].dropna(), lags=40, ax=plt.gca())
plt.show()

"""ACF and PACF plots for daily returns: doesn't provide much of insights or correlation has the results suggest no significant time dependent structure.

1.ACF plot: The ACF has a single spike at lag 0, and turn out to be zero for subsrquent lags, this indicates that the daily returns are likely uncorrelated over time.

PACF Plot: Similarly, PACF also shows sharp cutoff after lag 0, confirming that is no significant autocorreltion in the data after the initial lag.
futher suggest that daily returns are white noise or have very little memory of the past value
"""

import matplotlib.pyplot as plt
from datetime import timedelta
from pandas.plotting import register_matplotlib_converters
from statsmodels.tsa.stattools import acf, pacf
from statsmodels.tsa.arima_model import ARMA
register_matplotlib_converters()
from time import time



import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

plt.figure(figsize=(12, 6))
plt.subplot(211)
plot_acf(df['closing_price'].dropna(), lags=400, ax=plt.gca())
plt.subplot(212)
plot_pacf(df['closing_price'].dropna(), lags=40, ax=plt.gca())
plt.show()

"""ACF plot:
1. The ACF shows persistent correlation across many lags, which suggests that the closing prices are non-stationary. This is common with stock prices as they tend to show trends over time.
2. The sharp cutoff at lag 2 suggests that an AR(2) model might capture the autoregressive bheavior of the closing prie.

This suggests that to use ARIMA(2,1,0) model
2-for the autogressive term
1- for differncing
0- for moving average
"""

colsingdate = df.asfreq(pd.infer_freq(df['date']))

start_date = pd.to_datetime('2019-01-01')
end_date = pd.to_datetime('2023-01-01')
lim_df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]

plt.figure(figsize=(12, 6))
plt.plot(df['date'], df['closing_price'])
plt.xlabel('Date')
plt.ylabel('Closing Price')
plt.title('Closing Price Over Time')
plt.show()

"""Stationarity:
Substracting the pervious value from the current value. Now if we just difference onec, we might not get a seeries so we do that multiple times and the minimum number of the differncing operation required by the model, any stationary time series can be modeled with ARIMA models

ADF test:
We'll use the Augumented Dickey Fuller (ADF) test to check if the price series is stationary. The null hypothesis of the ADF test is that the time series is non- stationary. So, if the p-value  of the test is less than 0.06 then we can reject the null hypothesis and infer that the time series is indeed stationary.
"""

from statsmodels.tsa.stattools import adfuller
result = adfuller(df['closing_price'])
print('ADF Statistic:', result[0])
print('p-value:', result[1])
print('Critical Values:', result[4])

"""So, in our case p-value of 0.181 indicates that we fail to reject the null hypothesis of non-stationarity, as it is above the 0.05 significance level. The ADF statistic of -2.2716 is greater than the critical values at both the 5% (-2.8619) and 10% (-2.5669) levels, suggesting that the series is not stationary at these levels. Therefore, we conclude that the series is non-stationary"""





"""BY DIFFERNCING WE ARE TRYING TO BRING DATA TO STATIONARITY"""

df['closing_price_diff1'] = df['closing_price'].diff()
df = df.dropna(subset=['closing_price_diff1'])

# Plot the differenced series
plt.figure(figsize=(12, 6))
plt.plot(df['date'], df['closing_price_diff1'])
plt.xlabel('Date')
plt.ylabel('Closing Price (First Difference)')
plt.title('Closing Price After First Differencing')
plt.show()


# Augmented Dickey-Fuller test
from statsmodels.tsa.stattools import adfuller

result = adfuller(df['closing_price_diff1'])
print('ADF Statistic:', result[0])
print('p-value:', result[1])
print('Critical Values:', result[4])

!pip install statsmodels

import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import acf, pacf

acf_values = acf(df['closing_price_diff1'].dropna(), nlags=40)
pacf_values = pacf(df['closing_price_diff1'].dropna(), nlags=40)

# Create the ACF plot
plt.figure(figsize=(12, 4))
plt.bar(range(1, 41), acf_values[:40], color='blue', label='ACF')
plt.xlabel('Lag')
plt.ylabel('Autocorrelation')
plt.title('Autocorrelation Function (ACF)')
plt.legend()
plt.show()

# Create the PACF plot
plt.figure(figsize=(12, 4))
plt.bar(range(1, 41), pacf_values[:40], color='orange', label='PACF')
plt.xlabel('Lag')
plt.ylabel('Partial Autocorrelation')
plt.title('Partial Autocorrelation Function (PACF)')
plt.legend()
plt.show()

df['closing_price_diff'] = df['closing_price'].diff().diff()


df = df.dropna(subset=['closing_price_diff'])

# Plot the differenced series
plt.figure(figsize=(12, 6))
plt.plot(df['date'], df['closing_price_diff'])
plt.xlabel('Date')
plt.ylabel('Closing Price (First Difference)')
plt.title('Closing Price After First Differencing')
plt.show()


# Augmented Dickey-Fuller test
from statsmodels.tsa.stattools import adfuller

result = adfuller(df['closing_price_diff'])
print('ADF Statistic:', result[0])
print('p-value:', result[1])
print('Critical Values:', result[4])



acf_values = acf(df['closing_price_diff'].dropna(), nlags=40)
pacf_values = pacf(df['closing_price_diff'].dropna(), nlags=40)

# Create the ACF plot
plt.figure(figsize=(12, 4))  # Use plt.figure() here
plt.bar(range(1, 41), acf_values[:40], color='blue', label='ACF')
plt.xlabel('Lag')
plt.ylabel('Autocorrelation')
plt.title('Autocorrelation Function (ACF)')
plt.legend()
plt.show()

# Create the PACF plot
plt.figure(figsize=(12, 4))  # Use plt.figure() here
plt.bar(range(1, 41), pacf_values[:40], color='orange', label='PACF')
plt.xlabel('Lag')
plt.ylabel('Partial Autocorrelation')
plt.title('Partial Autocorrelation Function (PACF)')
plt.legend()
plt.show()

"""DIFFERENCING twice made PCAF give highly negetive value this is overdifferncing"""

import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

plt.figure(figsize=(12, 6))
plt.subplot(211)
plot_acf(df['daily_return'].dropna(), lags=40, ax=plt.gca())
plt.subplot(212)
plot_pacf(df['daily_return'].dropna(), lags=40, ax=plt.gca())
plt.show()

"""ACF and PACF plots for daily returns:
doesn't provide much of insights or correlation  has the results suggest no significant time dependent structure.

 1.ACF plot:
The ACF has a single spike at lag 0, and turn out to be zero for subsrquent lags, this indicates that the daily returns are likely uncorrelated over time.

2. PACF Plot:
Similarly, PACF also shows sharp cutoff after lag 0, confirming that is no significant autocorreltion in the data after the initial lag.

futher suggest that daily returns are white noise or have very little memory of the past value
"""

!pip install pmdarima

splt='2023-01-12'
train_df = df[df['date']<splt]
test_df = df[df['date']>=splt]
print(f"Training set size: {len(train_df)}")
print(f"Testing set size: {len(test_df)}")



import pandas as pd
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error
import numpy as np
import matplotlib.pyplot as plt


df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y')

train = df[df['date'] < '2023-01-12']['closing_price_diff1']
test = df[df['date'] >= '2023-01-12']['closing_price_diff1']


model = ARIMA(train, order=(2, 1, 0))
model_fit = model.fit()
predictions = model_fit.forecast(steps=len(test))


mse = mean_squared_error(test, predictions)
rmse = np.sqrt(mse)
print(f'Test RMSE: {rmse}')


plt.figure(figsize=(10, 6))
plt.plot(test.index, test, label='Actual')
plt.plot(test.index, predictions, label='Predicted', color='red')
plt.title('ARIMA Model: Predictions vs Actual Test Data')
plt.xlabel('Date')
plt.ylabel('Closing Price (First Difference)')
plt.legend()
plt.show()

"""Results:

Based on the above results, the ARIMA model is unable to predict market performance.

The following might be the reasons:

*   The data is highly volatile and skewed, affecting predictions

*   The inconsideration of market sentiment may have affected the results

Lets impliment LSTM :




"""

import numpy as np
from sklearn.preprocessing import MinMaxScaler

# Prepare the data for LSTM
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(df[['closing_price_diff1']].values)

# Create training and testing datasets
train_size = int(len(scaled_data) * 0.8)
train_data = scaled_data[:train_size]
test_data = scaled_data[train_size:]


def create_dataset(data, time_step=1):
    X, y = [], []
    for i in range(len(data) - time_step - 1):
        a = data[i:(i + time_step), 0]
        X.append(a)
        y.append(data[i + time_step, 0])
    return np.array(X), np.array(y)

time_step = 30
X_train, y_train = create_dataset(train_data, time_step)
X_test, y_test = create_dataset(test_data, time_step)


X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout

# Build the LSTM model
model_lstm = Sequential()
model_lstm.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)))
model_lstm.add(Dropout(0.2))
model_lstm.add(LSTM(50))
model_lstm.add(Dropout(0.2))
model_lstm.add(Dense(1))

model_lstm.compile(optimizer='adam', loss='mean_squared_error')
model_lstm.fit(X_train, y_train, epochs=100, batch_size=32)

predicted_prices = model_lstm.predict(X_test)
predicted_prices = scaler.inverse_transform(predicted_prices)
mse_lstm = mean_squared_error(y_test, predicted_prices)
rmse_lstm = np.sqrt(mse_lstm)
print(f'LSTM Test RMSE: {rmse_lstm}')


plt.figure(figsize=(10, 6))
plt.plot(scaler.inverse_transform(test_data[time_step + 1:]), label='Actual Prices')
plt.plot(predicted_prices, label='Predicted Prices', color='red')
plt.title('LSTM Model: Predictions vs Actual Test Data')
plt.xlabel('Time Steps')
plt.ylabel('Closing Price')
plt.legend()
plt.show()

"""This is the result from sol lstm model, with some kind of improvement in catching the spikes.

I wanted to restart whole process
"""

import pandas as pd
from statsmodels.tsa.arima.model import ARIMA


train = df[df['date'] < '2023-01-12']['closing_price_diff1']

# Fit ARIMA model
model_arima = ARIMA(train, order=(2, 1, 0))
model_fit = model_arima.fit()

# Get predictions and residuals
arima_predictions = model_fit.predict(start=len(train), end=len(train) + len(test) - 1, dynamic=False)
residuals = train - model_fit.fittedvalues
model_fit.summary()

from sklearn.preprocessing import MinMaxScaler

# Scale residuals for LSTM
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_residuals = scaler.fit_transform(residuals.values.reshape(-1, 1))

# Create dataset for LSTM
def create_dataset(data, time_step=1):
    X, y = [], []
    for i in range(len(data) - time_step - 1):
        a = data[i:(i + time_step), 0]
        X.append(a)
        y.append(data[i + time_step, 0])
    return np.array(X), np.array(y)

time_step = 30  # Adjust based on your needs
X_residuals, y_residuals = create_dataset(scaled_residuals, time_step)

# Reshape input to be [samples, time steps, features]
X_residuals = X_residuals.reshape(X_residuals.shape[0], X_residuals.shape[1], 1)

"""Building LSTM model

"""

from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout

model_lstm = Sequential()
model_lstm.add(LSTM(50, return_sequences=True, input_shape=(X_residuals.shape[1], 1)))
model_lstm.add(Dropout(0.2))
model_lstm.add(LSTM(50))
model_lstm.add(Dropout(0.2))
model_lstm.add(Dense(1))


model_lstm.compile(optimizer='adam', loss='mean_squared_error')


model_lstm.fit(X_residuals, y_residuals, epochs=100, batch_size=32)

predicted_residuals = model_lstm.predict(X_test)
predicted_residuals = scaler.inverse_transform(predicted_residuals)

print(f'Shape of ARIMA predictions: {arima_predictions.shape}')
print(f'Shape of LSTM predicted residuals: {predicted_residuals.shape}')

predicted_residuals = predicted_residuals[:len(arima_predictions)]

if len(predicted_residuals) < len(arima_predictions):
    padding = np.zeros(len(arima_predictions) - len(predicted_residuals))
    predicted_residuals = np.concatenate((predicted_residuals.flatten(), padding))

"""Integrating ARIMA and LSTM models"""

final_predictions = arima_predictions + predicted_residuals.flatten()

plt.figure(figsize=(10, 6))
plt.plot(test.index, test.values, label='Actual Prices')
plt.plot(test.index, final_predictions, label='Hybrid Model Predictions', color='red')
plt.title('Hybrid Model: Predictions vs Actual Test Data')
plt.xlabel('Date')
plt.ylabel('Closing Price')
plt.legend()
plt.show()

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

actual_values = test.values[:len(final_predictions)]  # Adjust based on length
predicted_values = final_predictions

# Calculate evaluation metrics
mae = mean_absolute_error(actual_values, predicted_values)
mse = mean_squared_error(actual_values, predicted_values)
rmse = np.sqrt(mse)
r2 = r2_score(actual_values, predicted_values)

# Print results
print(f'MAE: {mae}')
print(f'MSE: {mse}')
print(f'RMSE: {rmse}')
print(f'R-squared: {r2}')